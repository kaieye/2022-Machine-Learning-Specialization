{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Lab - Advice for Applying Machine Learning\n",
    "In this lab, you will explore techniques to evaluate and improve your machine learning models.\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Packages ](#1)\n",
    "- [ 2 - Evaluating a Learning Algorithm (Polynomial Regression)](#2)\n",
    "  - [ 2.1 Splitting your data set](#2.1)\n",
    "  - [ 2.2 Error calculation for model evaluation, linear regression](#2.2)\n",
    "    - [ Exercise 1](#ex01)\n",
    "  - [ 2.3 Compare performance on training and test data](#2.3)\n",
    "- [ 3 - Bias and Variance<img align=\"Right\" src=\"./images/C2_W3_BiasVarianceDegree.png\"  style=\" width:500px; padding: 10px 20px ; \"> ](#3)\n",
    "  - [ 3.1 Plot Train, Cross-Validation, Test](#3.1)\n",
    "  - [ 3.2 Finding the optimal degree](#3.2)\n",
    "  - [ 3.3 Tuning Regularization.](#3.3)\n",
    "  - [ 3.4 Getting more data: Increasing Training Set Size (m)](#3.4)\n",
    "- [ 4 - Evaluating a Learning Algorithm (Neural Network)](#4)\n",
    "  - [ 4.1 Data Set](#4.1)\n",
    "  - [ 4.2 Evaluating categorical model by calculating classification error](#4.2)\n",
    "    - [ Exercise 2](#ex02)\n",
    "- [ 5 - Model Complexity](#5)\n",
    "  - [ Exercise 3](#ex03)\n",
    "  - [ 5.1 Simple model](#5.1)\n",
    "    - [ Exercise 4](#ex04)\n",
    "- [ 6 - Regularization](#6)\n",
    "  - [ Exercise 5](#ex05)\n",
    "- [ 7 - Iterate to find optimal regularization value](#7)\n",
    "  - [ 7.1 Test](#7.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Packages \n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
    "- [numpy](https://numpy.org/) is the fundamental package for scientific computing Python.\n",
    "- [matplotlib](http://matplotlib.org) is a popular library to plot graphs in Python.\n",
    "- [scikitlearn](https://scikit-learn.org/stable/) is a basic library for data mining\n",
    "- [tensorflow](https://www.tensorflow.org/) a popular platform for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu,linear\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "from public_tests_a1 import * \n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from assigment_utils import *\n",
    "\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Evaluating a Learning Algorithm (Polynomial Regression)\n",
    "\n",
    "<img align=\"Right\" src=\"./images/C2_W3_TrainingVsNew.png\"  style=\" width:350px; padding: 10px 20px ; \"> Let's say you have created a machine learning model and you find it *fits* your training data very well. You're done? Not quite. The goal of creating the model was to be able to predict values for <span style=\"color:blue\">*new* </span> examples. \n",
    "\n",
    "How can you test your model's performance on new data before deploying it?   \n",
    "The answer has two parts:\n",
    "* Split your original data set into \"Training\" and \"Test\" sets. \n",
    "    * Use the training data to fit the parameters of the model\n",
    "    * Use the test data to evaluate the model on *new* data\n",
    "* Develop an error function to evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Splitting your data set\n",
    "Lectures advised reserving 20-40% of your data set for testing. Let's use an `sklearn` function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to perform the split. Double-check the shapes after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "X,y,x_ideal,y_ideal = gen_data(18, 2, 0.7)\n",
    "print(\"X.shape\", X.shape, \"y.shape\", y.shape)\n",
    "\n",
    "#split the data using sklearn routine \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=1)\n",
    "print(\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Plot Train, Test sets\n",
    "You can see below the data points that will be part of training (in red) are intermixed with those that the model is not trained on (test). This particular data set is a quadratic function with noise added. The \"ideal\" curve is shown for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "ax.set_title(\"Training, Test\",fontsize = 14)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "ax.scatter(X_test, y_test,   color = dlc[\"dlblue\"],   label=\"test\")\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Error calculation for model evaluation, linear regression\n",
    "When *evaluating* a linear regression model, you average the squared error difference of the predicted values and the target values.\n",
    "\n",
    "$$ J_\\text{test}(\\mathbf{w},b) = \n",
    "            \\frac{1}{2m_\\text{test}}\\sum_{i=0}^{m_\\text{test}-1} ( f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}_\\text{test}) - y^{(i)}_\\text{test} )^2 \n",
    "            \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1\n",
    "\n",
    "Below, create a function to evaluate the error on a data set for a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED CELL: eval_mse\n",
    "def eval_mse(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the mean squared error on a data set.\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:\n",
    "      err: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    err = 0.0\n",
    "    for i in range(m):\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array([2.4, 4.2])\n",
    "y_tmp = np.array([2.3, 4.1])\n",
    "eval_mse(y_hat, y_tmp)\n",
    "\n",
    "# BEGIN UNIT TEST\n",
    "test_eval_mse(eval_mse)   \n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "\n",
    "    \n",
    "```python\n",
    "def eval_mse(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the mean squared error on a data set.\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:\n",
    "      err: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    err = 0.0\n",
    "    for i in range(m):\n",
    "        err_i  = ( (yhat[i] - y[i])**2 ) \n",
    "        err   += err_i                                                                \n",
    "    err = err / (2*m)                    \n",
    "    return(err)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### 2.3 Compare performance on training and test data\n",
    "Let's build a high degree polynomial model to minimize training error. This will use the linear_regression functions from `sklearn`. The code is in the imported utility file if you would like to see the details. The steps below are:\n",
    "* create and fit the model. ('fit' is another name for training or running gradient descent).\n",
    "* compute the error on the training data.\n",
    "* compute the error on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model in sklearn, train on training data\n",
    "degree = 10\n",
    "lmodel = lin_model(degree)\n",
    "lmodel.fit(X_train, y_train)\n",
    "\n",
    "# predict on training data, find training error\n",
    "yhat = lmodel.predict(X_train)\n",
    "err_train = lmodel.mse(y_train, yhat)\n",
    "\n",
    "# predict on test data, find error\n",
    "yhat = lmodel.predict(X_test)\n",
    "err_test = lmodel.mse(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed error on the training set is substantially less than that of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training err {err_train:0.2f}, test err {err_test:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows why this is. The model fits the training data very well. To do so, it has created a complex function. The test data was not part of the training and the model does a poor job of predicting on this data.  \n",
    "This model would be described as 1) is overfitting, 2) has high variance 3) 'generalizes' poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions over data range \n",
    "x = np.linspace(0,int(X.max()),100)  # predict values for plot\n",
    "y_pred = lmodel.predict(x).reshape(-1,1)\n",
    "\n",
    "plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set error shows this model will not work well on new data. If you use the test error to guide improvements in the model, then the model will perform well on the test data... but the test data was meant to represent *new* data.\n",
    "You need yet another set of data to test new data performance.\n",
    "\n",
    "The proposal made during lecture is to separate data into three groups. The distribution of training, cross-validation and test sets shown in the below table is a typical distribution, but can be varied depending on the amount of data available.\n",
    "\n",
    "| data             | % of total | Description |\n",
    "|------------------|:----------:|:---------|\n",
    "| training         | 60         | Data used to tune model parameters $w$ and $b$ in training or fitting |\n",
    "| cross-validation | 20         | Data used to tune other model parameters like degree of polynomial, regularization or the architecture of a neural network.|\n",
    "| test             | 20         | Data used to test the model after tuning to gauge performance on new data |\n",
    "\n",
    "\n",
    "Let's generate three data sets below. We'll once again use `train_test_split` from `sklearn` but will call it twice to get three splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate  data\n",
    "X,y, x_ideal,y_ideal = gen_data(40, 5, 0.7)\n",
    "print(\"X.shape\", X.shape, \"y.shape\", y.shape)\n",
    "\n",
    "#split the data using sklearn routine \n",
    "X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.40, random_state=1)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.50, random_state=1)\n",
    "print(\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
    "print(\"X_cv.shape\", X_cv.shape, \"y_cv.shape\", y_cv.shape)\n",
    "print(\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Bias and Variance<img align=\"Right\" src=\"./images/C2_W3_BiasVarianceDegree.png\"  style=\" width:500px; padding: 10px 20px ; \"> \n",
    " Above, it was clear the degree of the polynomial model was too high. How can you choose a good value? It turns out, as shown in the diagram, the training and cross-validation performance can provide guidance. By trying a range of degree values, the training and cross-validation performance can be evaluated. As the degree becomes too large, the cross-validation performance will start to degrade relative to the training performance. Let's try this on our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Plot Train, Cross-Validation, Test\n",
    "You can see below the datapoints that will be part of training (in red) are intermixed with those that the model is not trained on (test and cv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "ax.set_title(\"Training, CV, Test\",fontsize = 14)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "ax.scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], label=\"cv\")\n",
    "ax.scatter(X_test, y_test,   color = dlc[\"dlblue\"],   label=\"test\")\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Finding the optimal degree\n",
    "In previous labs, you found that you could create a model capable of fitting complex curves by utilizing a polynomial (See Course1, Week2 Feature Engineering and Polynomial Regression Lab).  Further, you demonstrated that by increasing the *degree* of the polynomial, you could *create* overfitting. (See Course 1, Week3, Over-Fitting Lab). Let's use that knowledge here to test our ability to tell the difference between over-fitting and under-fitting.\n",
    "\n",
    "Let's train the model repeatedly, increasing the degree of the polynomial each iteration. Here, we're going to use the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) linear regression model for speed and simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 9\n",
    "err_train = np.zeros(max_degree)    \n",
    "err_cv = np.zeros(max_degree)      \n",
    "x = np.linspace(0,int(X.max()),100)  \n",
    "y_pred = np.zeros((100,max_degree))  #columns are lines to plot\n",
    "\n",
    "for degree in range(max_degree):\n",
    "    lmodel = lin_model(degree+1)\n",
    "    lmodel.fit(X_train, y_train)\n",
    "    yhat = lmodel.predict(X_train)\n",
    "    err_train[degree] = lmodel.mse(y_train, yhat)\n",
    "    yhat = lmodel.predict(X_cv)\n",
    "    err_cv[degree] = lmodel.mse(y_cv, yhat)\n",
    "    y_pred[:,degree] = lmodel.predict(x)\n",
    "    \n",
    "optimal_degree = np.argmin(err_cv)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Let's plot the result:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt_optimal_degree(X_train, y_train, X_cv, y_cv, x, y_pred, x_ideal, y_ideal, \n",
    "                   err_train, err_cv, optimal_degree, max_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above demonstrates that separating data into two groups, data the model is trained on and data the model has not been trained on, can be used to determine if the model is underfitting or overfitting. In our example, we created a variety of models varying from underfitting to overfitting by increasing the degree of the polynomial used. \n",
    "- On the left plot, the solid lines represent the predictions from these models. A polynomial model with degree 1 produces a straight line that intersects very few data points, while the maximum degree hews very closely to every data point. \n",
    "- on the right:\n",
    "    - the error on the trained data (blue) decreases as the model complexity increases as expected\n",
    "    - the error of the cross-validation data decreases initially as the model starts to conform to the data, but then increases as the model starts to over-fit on the training data (fails to *generalize*).     \n",
    "    \n",
    "It's worth noting that the curves in these examples as not as smooth as one might draw for a lecture. It's clear the specific data points assigned to each group can change your results significantly. The general trend is what is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Tuning Regularization.\n",
    "In previous labs, you have utilized *regularization* to reduce overfitting. Similar to degree, one can use the same methodology to tune the regularization parameter lambda ($\\lambda$).\n",
    "\n",
    "Let's demonstrate this by starting with a high degree polynomial and varying the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_range = np.array([0.0, 1e-6, 1e-5, 1e-4,1e-3,1e-2, 1e-1,1,10,100])\n",
    "num_steps = len(lambda_range)\n",
    "degree = 10\n",
    "err_train = np.zeros(num_steps)    \n",
    "err_cv = np.zeros(num_steps)       \n",
    "x = np.linspace(0,int(X.max()),100) \n",
    "y_pred = np.zeros((100,num_steps))  #columns are lines to plot\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lambda_= lambda_range[i]\n",
    "    lmodel = lin_model(degree, regularization=True, lambda_=lambda_)\n",
    "    lmodel.fit(X_train, y_train)\n",
    "    yhat = lmodel.predict(X_train)\n",
    "    err_train[i] = lmodel.mse(y_train, yhat)\n",
    "    yhat = lmodel.predict(X_cv)\n",
    "    err_cv[i] = lmodel.mse(y_cv, yhat)\n",
    "    y_pred[:,i] = lmodel.predict(x)\n",
    "    \n",
    "optimal_reg_idx = np.argmin(err_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt_tune_regularization(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, optimal_reg_idx, lambda_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the plots show that as regularization increases, the model moves from a high variance (overfitting) model to a high bias (underfitting) model. The vertical line in the right plot shows the optimal value of lambda. In this example, the polynomial degree was set to 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Getting more data: Increasing Training Set Size (m)\n",
    "When a model is overfitting (high variance), collecting additional data can improve performance. Let's try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range,degree = tune_m()\n",
    "plt_tune_m(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots show that when a model has high variance and is overfitting, adding more examples improves performance. Note the curves on the left plot. The final curve with the highest value of $m$ is a smooth curve that is in the center of the data. On the right, as the number of examples increases, the performance of the training set and cross-validation set converge to similar values. Note that the curves are not as smooth as one might see in a lecture. That is to be expected. The trend remains clear: more data improves generalization. \n",
    "\n",
    "> Note that adding more examples when the model has high bias (underfitting) does not improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Evaluating a Learning Algorithm (Neural Network)\n",
    "Above, you tuned aspects of a polynomial regression model. Here, you will work with a neural network model. Let's start by creating a classification data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### 4.1 Data Set\n",
    "Run the cell below to generate a data set and split it into training, cross-validation (CV) and test sets. In this example, we're increasing the percentage of cross-validation data points for emphasis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and split data set\n",
    "X, y, centers, classes, std = gen_blobs()\n",
    "\n",
    "# split the data. Large CV population for demonstration\n",
    "X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.50, random_state=1)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.20, random_state=1)\n",
    "print(\"X_train.shape:\", X_train.shape, \"X_cv.shape:\", X_cv.shape, \"X_test.shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_train_eq_dist(X_train, y_train,classes, X_cv, y_cv, centers, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see the data on the left. There are six clusters identified by color. Both training points (dots) and cross-validataion points (triangles) are shown. The interesting points are those that fall in ambiguous locations where either cluster might consider them members. What would you expect a neural network model to do? What would be an example of overfitting? underfitting?  \n",
    "On the right is an example of an 'ideal' model, or a model one might create knowing the source of the data. The lines represent 'equal distance' boundaries where the distance between center points is equal. It's worth noting that this model would \"misclassify\" roughly 8% of the total data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### 4.2 Evaluating categorical model by calculating classification error\n",
    "The evaluation function for categorical models used here is simply the fraction of incorrect predictions:  \n",
    "$$ J_{cv} =\\frac{1}{m}\\sum_{i=0}^{m-1} \n",
    "\\begin{cases}\n",
    "    1, & \\text{if $\\hat{y}^{(i)} \\neq y^{(i)}$}\\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<a name=\"ex02\"></a>\n",
    "### Exercise 2\n",
    "\n",
    "Below, complete the routine to calculate classification error. Note, in this lab, target values are the index of the category and are not [one-hot encoded](https://en.wikipedia.org/wiki/One-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED CELL: eval_cat_err\n",
    "def eval_cat_err(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the categorization error\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:|\n",
    "      cerr: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    incorrect = 0\n",
    "    for i in range(m):\n",
    "    ### START CODE HERE ### \n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return(cerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array([1, 2, 0])\n",
    "y_tmp = np.array([1, 2, 3])\n",
    "print(f\"categorization error {np.squeeze(eval_cat_err(y_hat, y_tmp)):0.3f}, expected:0.333\" )\n",
    "y_hat = np.array([[1], [2], [0], [3]])\n",
    "y_tmp = np.array([[1], [2], [1], [3]])\n",
    "print(f\"categorization error {np.squeeze(eval_cat_err(y_hat, y_tmp)):0.3f}, expected:0.250\" )\n",
    "\n",
    "# BEGIN UNIT TEST  \n",
    "test_eval_cat_err(eval_cat_err)\n",
    "# END UNIT TEST\n",
    "# BEGIN UNIT TEST  \n",
    "test_eval_cat_err(eval_cat_err)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "```python\n",
    "def eval_cat_err(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the categorization error\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:|\n",
    "      cerr: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    incorrect = 0\n",
    "    for i in range(m):\n",
    "        if yhat[i] != y[i]:    # @REPLACE\n",
    "            incorrect += 1     # @REPLACE\n",
    "    cerr = incorrect/m         # @REPLACE\n",
    "    return(cerr)                                    \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Model Complexity\n",
    "Below, you will build two models. A complex model and a simple model. You will evaluate the models to determine if they are likely to overfit or underfit.\n",
    "\n",
    "###  5.1 Complex model\n",
    "\n",
    "<a name=\"ex03\"></a>\n",
    "### Exercise 3\n",
    "Below, compose a three-layer model:\n",
    "* Dense layer with 120 units, relu activation\n",
    "* Dense layer with 40 units, relu activation\n",
    "* Dense layer with 6 units and a linear activation (not softmax)  \n",
    "Compile using\n",
    "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
    "* Adam optimizer with learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED CELL: model\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential(\n",
    "    [\n",
    "        ### START CODE HERE ### \n",
    "  \n",
    "        ### END CODE HERE ### \n",
    "\n",
    "    ], name=\"Complex\"\n",
    ")\n",
    "model.compile(\n",
    "    ### START CODE HERE ### \n",
    "    loss=None,\n",
    "    optimizer=None,\n",
    "    ### END CODE HERE ### \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1000\n",
    ")\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "model.summary()\n",
    "\n",
    "model_test(model, classes, X_train.shape[1]) \n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "Summary should match this (layer instance names may increment )\n",
    "```\n",
    "Model: \"Complex\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "L1 (Dense)                   (None, 120)               360       \n",
    "_________________________________________________________________\n",
    "L2 (Dense)                   (None, 40)                4840      \n",
    "_________________________________________________________________\n",
    "L3 (Dense)                   (None, 6)                 246       \n",
    "=================================================================\n",
    "Total params: 5,446\n",
    "Trainable params: 5,446\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "  <details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for more hints</b></font></summary>\n",
    "  \n",
    "```python\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(120, activation = 'relu', name = \"L1\"),      \n",
    "        Dense(40, activation = 'relu', name = \"L2\"),         \n",
    "        Dense(classes, activation = 'linear', name = \"L3\")  \n",
    "    ], name=\"Complex\"\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),          \n",
    "    optimizer=tf.keras.optimizers.Adam(0.01),   \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=1000\n",
    ")                                  \n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a model for plotting routines to call\n",
    "model_predict = lambda Xl: np.argmax(tf.nn.softmax(model.predict(Xl)).numpy(),axis=1)\n",
    "plt_nn(model_predict,X_train,y_train, classes, X_cv, y_cv, suptitle=\"Complex Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has worked very hard to capture outliers of each category. As a result, it has miscategorized some of the cross-validation data. Let's calculate the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cerr_complex = eval_cat_err(y_train, model_predict(X_train))\n",
    "cv_cerr_complex = eval_cat_err(y_cv, model_predict(X_cv))\n",
    "print(f\"categorization error, training, complex model: {training_cerr_complex:0.3f}\")\n",
    "print(f\"categorization error, cv,       complex model: {cv_cerr_complex:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.1\"></a>\n",
    "### 5.1 Simple model\n",
    "Now, let's try a simple model\n",
    "\n",
    "<a name=\"ex04\"></a>\n",
    "### Exercise 4\n",
    "\n",
    "Below, compose a two-layer model:\n",
    "* Dense layer with 6 units, relu activation\n",
    "* Dense layer with 6 units and a linear activation. \n",
    "Compile using\n",
    "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
    "* Adam optimizer with learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED CELL: model_s\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "model_s = Sequential(\n",
    "    [\n",
    "        ### START CODE HERE ### \n",
    "      \n",
    "        ### END CODE HERE ### \n",
    "    ], name = \"Simple\"\n",
    ")\n",
    "model_s.compile(\n",
    "    ### START CODE HERE ### \n",
    "    loss=None,\n",
    "    optimizer=None,\n",
    "    ### START CODE HERE ### \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# BEGIN UNIT TEST\n",
    "model_s.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=1000\n",
    ")\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "model_s.summary()\n",
    "\n",
    "model_s_test(model_s, classes, X_train.shape[1])\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "Summary should match this (layer instance names may increment )\n",
    "```\n",
    "Model: \"Simple\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "L1 (Dense)                   (None, 6)                 18        \n",
    "_________________________________________________________________\n",
    "L2 (Dense)                   (None, 6)                 42        \n",
    "=================================================================\n",
    "Total params: 60\n",
    "Trainable params: 60\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "  <details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for more hints</b></font></summary>\n",
    "  \n",
    "```python\n",
    "tf.random.set_seed(1234)\n",
    "model_s = Sequential(\n",
    "    [\n",
    "        Dense(6, activation = 'relu', name=\"L1\"),            # @REPLACE\n",
    "        Dense(classes, activation = 'linear', name=\"L2\")     # @REPLACE\n",
    "    ], name = \"Simple\"\n",
    ")\n",
    "model_s.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),     # @REPLACE\n",
    "    optimizer=tf.keras.optimizers.Adam(0.01),     # @REPLACE\n",
    ")\n",
    "\n",
    "model_s.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=1000\n",
    ")                                   \n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a model for plotting routines to call\n",
    "model_predict_s = lambda Xl: np.argmax(tf.nn.softmax(model_s.predict(Xl)).numpy(),axis=1)\n",
    "plt_nn(model_predict_s,X_train,y_train, classes, X_cv, y_cv, suptitle=\"Simple Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple models does pretty well. Let's calculate the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cerr_simple = eval_cat_err(y_train, model_predict_s(X_train))\n",
    "cv_cerr_simple = eval_cat_err(y_cv, model_predict_s(X_cv))\n",
    "print(f\"categorization error, training, simple model, {training_cerr_simple:0.3f}, complex model: {training_cerr_complex:0.3f}\" )\n",
    "print(f\"categorization error, cv,       simple model, {cv_cerr_simple:0.3f}, complex model: {cv_cerr_complex:0.3f}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple model has a little higher classification error on training data but does better on cross-validation data than the more complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Regularization\n",
    "As in the case of polynomial regression, one can apply regularization to moderate the impact of a more complex model. Let's try this below.\n",
    "\n",
    "<a name=\"ex05\"></a>\n",
    "### Exercise 5\n",
    "\n",
    "Reconstruct your complex model, but this time include regularization.\n",
    "Below, compose a three-layer model:\n",
    "* Dense layer with 120 units, relu activation, `kernel_regularizer=tf.keras.regularizers.l2(0.1)`\n",
    "* Dense layer with 40 units, relu activation, `kernel_regularizer=tf.keras.regularizers.l2(0.1)`\n",
    "* Dense layer with 6 units and a linear activation. \n",
    "Compile using\n",
    "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
    "* Adam optimizer with learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED CELL: model_r\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "model_r = Sequential(\n",
    "    [\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "    ], name= None\n",
    ")\n",
    "model_r.compile(\n",
    "    ### START CODE HERE ### \n",
    "    loss=None,\n",
    "    optimizer=None,\n",
    "    ### START CODE HERE ### \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "model_r.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1000\n",
    ")\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "model_r.summary()\n",
    "\n",
    "model_r_test(model_r, classes, X_train.shape[1]) \n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "Summary should match this (layer instance names may increment )\n",
    "```\n",
    "Model: \"ComplexRegularized\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "L1 (Dense)                   (None, 120)               360       \n",
    "_________________________________________________________________\n",
    "L2 (Dense)                   (None, 40)                4840      \n",
    "_________________________________________________________________\n",
    "L3 (Dense)                   (None, 6)                 246       \n",
    "=================================================================\n",
    "Total params: 5,446\n",
    "Trainable params: 5,446\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "  <details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for more hints</b></font></summary>\n",
    "  \n",
    "```python\n",
    "tf.random.set_seed(1234)\n",
    "model_r = Sequential(\n",
    "    [\n",
    "        Dense(120, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), name=\"L1\"), \n",
    "        Dense(40, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), name=\"L2\"),  \n",
    "        Dense(classes, activation = 'linear', name=\"L3\")  \n",
    "    ], name=\"ComplexRegularized\"\n",
    ")\n",
    "model_r.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    optimizer=tf.keras.optimizers.Adam(0.01),                             \n",
    ")\n",
    "\n",
    "model_r.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=1000\n",
    ")                                   \n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a model for plotting routines to call\n",
    "model_predict_r = lambda Xl: np.argmax(tf.nn.softmax(model_r.predict(Xl)).numpy(),axis=1)\n",
    " \n",
    "plt_nn(model_predict_r, X_train,y_train, classes, X_cv, y_cv, suptitle=\"Regularized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look very similar to the 'ideal' model. Let's check classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cerr_reg = eval_cat_err(y_train, model_predict_r(X_train))\n",
    "cv_cerr_reg = eval_cat_err(y_cv, model_predict_r(X_cv))\n",
    "test_cerr_reg = eval_cat_err(y_test, model_predict_r(X_test))\n",
    "print(f\"categorization error, training, regularized: {training_cerr_reg:0.3f}, simple model, {training_cerr_simple:0.3f}, complex model: {training_cerr_complex:0.3f}\" )\n",
    "print(f\"categorization error, cv,       regularized: {cv_cerr_reg:0.3f}, simple model, {cv_cerr_simple:0.3f}, complex model: {cv_cerr_complex:0.3f}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple model is a bit better in the training set than the regularized model but it worse in the cross validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "## 7 - Iterate to find optimal regularization value\n",
    "As you did in linear regression, you can try many regularization values. This code takes several minutes to run. If you have time, you can run it and check the results. If not, you have completed the graded parts of the assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "models=[None] * len(lambdas)\n",
    "for i in range(len(lambdas)):\n",
    "    lambda_ = lambdas[i]\n",
    "    models[i] =  Sequential(\n",
    "        [\n",
    "            Dense(120, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "            Dense(40, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "            Dense(classes, activation = 'linear')\n",
    "        ]\n",
    "    )\n",
    "    models[i].compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "    )\n",
    "\n",
    "    models[i].fit(\n",
    "        X_train,y_train,\n",
    "        epochs=1000\n",
    "    )\n",
    "    print(f\"Finished lambda = {lambda_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regularization is increased, the performance of the model on the training and cross-validation data sets converge. For this data set and model, lambda > 0.01 seems to be a reasonable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7.1\"></a>\n",
    "### 7.1 Test\n",
    "Let's try our optimized models on the test set and compare them to 'ideal' performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_compare(X_test,y_test, classes, model_predict_s, model_predict_r, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test set is small and seems to have a number of outliers so classification error is high. However, the performance of our optimized models is comparable to ideal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! \n",
    "You have become familiar with important tools to apply when evaluating your machine learning models. Namely:  \n",
    "* splitting data into trained and untrained sets allows you to differentiate between underfitting and overfitting\n",
    "* creating three data sets, Training, Cross-Validation and Test allows you to\n",
    "    * train your parameters $W,B$ with the training set\n",
    "    * tune model parameters such as complexity, regularization and number of examples with the cross-validation set\n",
    "    * evaluate your 'real world' performance using the test set.\n",
    "* comparing training vs cross-validation performance provides insight into a model's propensity towards overfitting (high variance) or underfitting (high bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
